{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CyberTruck_2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkolln/DataChallenge1/blob/master/CyberTruck_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLbLXouF2R4w",
        "colab_type": "code",
        "outputId": "b6e72e2b-8a34-4407-eacb-2fee4d13366e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/fchollet/keras.git\n",
            "  Cloning git://github.com/fchollet/keras.git to /tmp/pip-req-build-11milzv_\n",
            "  Running command git clone -q git://github.com/fchollet/keras.git /tmp/pip-req-build-11milzv_\n",
            "Building wheels for collected packages: Keras\n",
            "  Building wheel for Keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Keras: filename=Keras-2.3.1-cp36-none-any.whl size=365121 sha256=3354111abcbe25fdad50325bfeaf9f87c8a8754dd7b575bc92d0553ac9770510\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m1mywfhp/wheels/dc/a7/a2/8b2d0fd23dee9c609b4c95f2b5ed27997ed17ccbeabbebfc64\n",
            "Successfully built Keras\n",
            "Installing collected packages: Keras\n",
            "  Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "Successfully installed Keras-2.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKLzpLdn43DX",
        "colab_type": "text"
      },
      "source": [
        "Importing all necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFeECHJ52aod",
        "colab_type": "code",
        "outputId": "54fd3422-c874-40cc-99ad-9b39b521e02b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import itertools\n",
        "import multiprocessing.pool\n",
        "import threading\n",
        "from functools import partial\n",
        "from keras.models import load_model\n",
        "from keras.models import Model\n",
        "import keras\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import layers, models\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.backend import relu, sigmoid\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from skimage import exposure, color\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.saved_model import builder as saved_model_builder\n",
        "from tensorflow.python.saved_model import utils\n",
        "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
        "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
        "from tensorflow.contrib.session_bundle import exporter\n",
        "import os\n",
        "%matplotlib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot as plt\n",
        "from numpy import loadtxt\n",
        "from keras.models import load_model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using matplotlib backend: agg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRmR7LO-5ENV",
        "colab_type": "text"
      },
      "source": [
        "#Setting up the model\n",
        "\n",
        "Creating a Sequential Model with Keras which has 3 convolutional layers, 2 max pooling layers in order to down sample input representations. Furthermore, relu activations is used to increase non-linearity in one of the dense layers. In the other dense layer, softmax is used as activation.\n",
        "\n",
        "Also, Adam learning rate optimizer is used with a small starting value of 0.01. \n",
        "\n",
        "As a loss-function categorical crossentropy is being used.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOhVJxn82f48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(labels_dim):\n",
        "    \"\"\"Create a Keras Sequential model with layers.\"\"\"\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                     activation='relu',\n",
        "                     input_shape=(128, 128, 3)))\n",
        "    #model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='glorot_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(labels_dim, activation='softmax', kernel_initializer='glorot_uniform'))\n",
        "\n",
        "    compile_model(model)\n",
        "    return model\n",
        "\n",
        "def compile_model(model):\n",
        "    opt =keras.optimizers.SGD(lr=0.001, momentum=0.99, decay=0.01)\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def read_train_data():\n",
        "    start_time = time.time()\n",
        "    print(\"Start Read Train Data\")\n",
        "    data = np.load(\"trainDataSmall.npz\")\n",
        "    #data = np.load(\"/content/drive/My Drive/Colab Notebooks/trainDataSmall.npz\")\n",
        "    print(\"Train data read --- %s seconds ---\" % (time.time() - start_time))\n",
        "    X_train = data[\"X_train\"]\n",
        "    Y_train = data[\"Y_train\"]\n",
        "    print(\"Training - Total examples per class\", np.sum(Y_train, axis=0))\n",
        "    return [X_train, Y_train]\n",
        "\n",
        "def read_test_data():\n",
        "    start_time = time.time()\n",
        "    print(\"Start Read Test Data\")\n",
        "    data = np.load(\"testDataSmall.npz\")\n",
        "    print(\"Test data read --- %s seconds ---\" % (time.time() - start_time))\n",
        "    X_test = data[\"X_test\"]\n",
        "    Y_test = data[\"Y_test\"]\n",
        "    print(\"Testing - Total examples per class\", np.sum(Y_test, axis=0))\n",
        "    return [X_test, Y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6hAy7Bo5hot",
        "colab_type": "text"
      },
      "source": [
        "Downloading the training and test data from the TU/e server."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoAZpjGK2hoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "![ -f testDataSmall.npz ] || wget -O testDataSmall.npz \"https://www.win.tue.nl/~cdecampos/testDataSmall.npz\"\n",
        "![ -f trainDataSmall.npz ] || wget -O trainDataSmall.npz \"https://www.win.tue.nl/~cdecampos/trainDataSmall.npz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KES4PgY_AACM",
        "colab_type": "text"
      },
      "source": [
        "Implementing adaptive histogram equalization as a preprocessing tool in the continous evaluation class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq5ehdH52ku2",
        "colab_type": "code",
        "outputId": "05ca0f31-526b-424f-9a61-e8251409a71b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def AHE(image):\n",
        "    image = exposure.equalize_adapthist(image, clip_limit=0.01)\n",
        "    \n",
        "                    \n",
        "datagen1 = ImageDataGenerator(\n",
        "        shear_range=0,\n",
        "        zoom_range=0,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function=AHE\n",
        "        )\n",
        "\n",
        "class ContinuousEval(keras.callbacks.Callback):\n",
        "    \"\"\"Continuous eval callback to evaluate the checkpoint once\n",
        "       every so many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 eval_frequency,\n",
        "                 job_dir):\n",
        "        self.eval_frequency = eval_frequency\n",
        "        self.job_dir = job_dir\n",
        "        [self.X_test, self.Y_test] = read_test_data()\n",
        "        datagen1.fit(self.X_test)\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        if epoch > 0 and epoch % self.eval_frequency == 0:\n",
        "            # Unhappy hack to work around h5py not being able to write to GCS.\n",
        "            # Force snapshots and saves to local filesystem, then copy them over to GCS.\n",
        "            model_path_glob = 'checkpoint.*'\n",
        "            model_path_glob = os.path.join(self.job_dir, model_path_glob)\n",
        "            checkpoints = glob.glob(model_path_glob)\n",
        "            if len(checkpoints) > 0:\n",
        "                checkpoints.sort()\n",
        "                retinopathy_model = load_model(checkpoints[-1])\n",
        "                retinopathy_model = compile_model(retinopathy_model)\n",
        "                loss, acc = retinopathy_model.evaluate(\n",
        "                    self.X_test, self.Y_test)\n",
        "                print('\\nEvaluation epoch[{}] metrics[{:.2f}, {:.2f}] {}'.format(\n",
        "                    epoch, loss, acc, retinopathy_model.metrics_names))\n",
        "            else:\n",
        "                print('\\nEvaluation epoch[{}] (no checkpoints found)'.format(epoch))\n",
        "                \n",
        "# Continuous eval callback\n",
        "evaluation = ContinuousEval(5, 'job_dir')\n",
        "validation=(evaluation.X_test, evaluation.Y_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Read Test Data\n",
            "Test data read --- 0.0014848709106445312 seconds ---\n",
            "Testing - Total examples per class [500. 500. 500. 500. 500.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gzGu1a1AP11",
        "colab_type": "text"
      },
      "source": [
        "# Training the model\n",
        "The model used # epochs and # batch size. Furthermore, some image augmentation techniques are implemented in the image-generator function. In the end, two plots are produced which show the accuracy and loss per epoch for the test and train data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37q78Ghi2nNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This code implements a Feed forward neural network using Keras API.\"\"\"\n",
        "\n",
        "CLASS_SIZE = 5\n",
        "FILE_PATH = 'checkpoint.{epoch:02d}.hdf5'\n",
        "RETINOPATHY_MODEL = 'retinopathy.hdf5'\n",
        "\n",
        "def run():\n",
        "    tf.keras.backend.clear_session()\n",
        "    #local dir to write checkpoints and export model\n",
        "    job_dir = 'jobdir'\n",
        "    #Batch size for training steps\n",
        "    train_batch_size = 100\n",
        "    #Maximum number of epochs on which to train\n",
        "    num_epochs = 50\n",
        "    #Checkpoint per n training epochs\n",
        "    checkpoint_epochs = 5\n",
        "    #Perform one evaluation per n epochs\n",
        "    eval_frequency = 5\n",
        "\n",
        "    retinopathy_model = model_fn(CLASS_SIZE)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(job_dir)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Unhappy hack to work around h5py not being able to write to GCS.\n",
        "    # Force snapshots and saves to local filesystem, then copy them over to GCS.\n",
        "    checkpoint_path = FILE_PATH\n",
        "    checkpoint_path = os.path.join(job_dir, checkpoint_path)\n",
        "\n",
        "    # Model checkpoint callback\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_path,\n",
        "        monitor='val_loss',\n",
        "        verbose=2,\n",
        "        period=checkpoint_epochs,\n",
        "        mode='max')\n",
        "\n",
        "    # Tensorboard logs callback\n",
        "    tblog = keras.callbacks.TensorBoard(\n",
        "        log_dir=os.path.join(job_dir, 'logs'),\n",
        "        histogram_freq=0,\n",
        "        write_graph=True,\n",
        "        embeddings_freq=0)\n",
        "\n",
        "    callbacks = [checkpoint, evaluation, tblog]\n",
        "\n",
        "    [X_train, Y_train] = read_train_data()\n",
        "    datagen1.fit(X_train)\n",
        "    \n",
        "    datagen = ImageDataGenerator(\n",
        "        shear_range=0,\n",
        "        zoom_range=0,\n",
        "        horizontal_flip=True)\n",
        "    \n",
        "    history = retinopathy_model.fit_generator(\n",
        "        datagen.flow(X_train, Y_train, batch_size=train_batch_size),\n",
        "        steps_per_epoch=100,\n",
        "        epochs=num_epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose=2,\n",
        "        validation_data=validation)\n",
        "\n",
        "    retinopathy_model.save(os.path.join(job_dir, RETINOPATHY_MODEL))\n",
        "    retinopathy_model.summary()\n",
        "    \n",
        "    # Creating a plot for loss and accuracy for both training and validation set.\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'], 'bo')\n",
        "    plt.plot(history.history['val_accuracy'], 'b')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_acc', 'test_acc'], loc='upper left')\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'], 'bo')\n",
        "    plt.plot(history.history['val_loss'], 'b')\n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_loss', 'test_loss'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGK3Gua32rgp",
        "colab_type": "code",
        "outputId": "bc6edc6b-2ae7-4444-a63a-e1782a3783f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!rm -fr jobdir/\n",
        "run()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Start Read Train Data\n",
            "Train data read --- 0.0005426406860351562 seconds ---\n",
            "Training - Total examples per class [1000. 1000. 1000. 1000. 1000.]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/50\n",
            " - 16s - loss: 1.6104 - accuracy: 0.2117 - val_loss: 1.6069 - val_accuracy: 0.2480\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/50\n",
            " - 12s - loss: 1.6049 - accuracy: 0.2350 - val_loss: 1.6017 - val_accuracy: 0.2368\n",
            "Epoch 3/50\n",
            " - 13s - loss: 1.5997 - accuracy: 0.2386 - val_loss: 1.5967 - val_accuracy: 0.2492\n",
            "Epoch 4/50\n",
            " - 13s - loss: 1.5930 - accuracy: 0.2584 - val_loss: 1.5871 - val_accuracy: 0.2936\n",
            "Epoch 5/50\n",
            " - 13s - loss: 1.5864 - accuracy: 0.2695 - val_loss: 1.5761 - val_accuracy: 0.2876\n",
            "\n",
            "Epoch 00005: saving model to jobdir/checkpoint.05.hdf5\n",
            "Epoch 6/50\n",
            "\n",
            "Evaluation epoch[5] (no checkpoints found)\n",
            " - 13s - loss: 1.5749 - accuracy: 0.2770 - val_loss: 1.5599 - val_accuracy: 0.3076\n",
            "Epoch 7/50\n",
            " - 13s - loss: 1.5661 - accuracy: 0.2851 - val_loss: 1.5445 - val_accuracy: 0.3168\n",
            "Epoch 8/50\n",
            " - 13s - loss: 1.5529 - accuracy: 0.2945 - val_loss: 1.5301 - val_accuracy: 0.3340\n",
            "Epoch 9/50\n",
            " - 13s - loss: 1.5446 - accuracy: 0.3013 - val_loss: 1.5173 - val_accuracy: 0.3236\n",
            "Epoch 10/50\n",
            " - 13s - loss: 1.5318 - accuracy: 0.3019 - val_loss: 1.4957 - val_accuracy: 0.3396\n",
            "\n",
            "Epoch 00010: saving model to jobdir/checkpoint.10.hdf5\n",
            "Epoch 11/50\n",
            "\n",
            "Evaluation epoch[10] (no checkpoints found)\n",
            " - 13s - loss: 1.5240 - accuracy: 0.3080 - val_loss: 1.4843 - val_accuracy: 0.3368\n",
            "Epoch 12/50\n",
            " - 13s - loss: 1.5150 - accuracy: 0.3141 - val_loss: 1.4694 - val_accuracy: 0.3476\n",
            "Epoch 13/50\n",
            " - 13s - loss: 1.5105 - accuracy: 0.3204 - val_loss: 1.4610 - val_accuracy: 0.3420\n",
            "Epoch 14/50\n",
            " - 13s - loss: 1.5062 - accuracy: 0.3241 - val_loss: 1.4593 - val_accuracy: 0.3472\n",
            "Epoch 15/50\n",
            " - 13s - loss: 1.5041 - accuracy: 0.3183 - val_loss: 1.4516 - val_accuracy: 0.3476\n",
            "\n",
            "Epoch 00015: saving model to jobdir/checkpoint.15.hdf5\n",
            "Epoch 16/50\n",
            "\n",
            "Evaluation epoch[15] (no checkpoints found)\n",
            " - 13s - loss: 1.4979 - accuracy: 0.3272 - val_loss: 1.4429 - val_accuracy: 0.3524\n",
            "Epoch 17/50\n",
            " - 13s - loss: 1.4926 - accuracy: 0.3249 - val_loss: 1.4383 - val_accuracy: 0.3484\n",
            "Epoch 18/50\n",
            " - 13s - loss: 1.4874 - accuracy: 0.3328 - val_loss: 1.4314 - val_accuracy: 0.3500\n",
            "Epoch 19/50\n",
            " - 13s - loss: 1.4824 - accuracy: 0.3334 - val_loss: 1.4278 - val_accuracy: 0.3604\n",
            "Epoch 20/50\n",
            " - 13s - loss: 1.4848 - accuracy: 0.3279 - val_loss: 1.4278 - val_accuracy: 0.3520\n",
            "\n",
            "Epoch 00020: saving model to jobdir/checkpoint.20.hdf5\n",
            "Epoch 21/50\n",
            "\n",
            "Evaluation epoch[20] (no checkpoints found)\n",
            " - 13s - loss: 1.4796 - accuracy: 0.3333 - val_loss: 1.4198 - val_accuracy: 0.3608\n",
            "Epoch 22/50\n",
            " - 13s - loss: 1.4771 - accuracy: 0.3367 - val_loss: 1.4323 - val_accuracy: 0.3464\n",
            "Epoch 23/50\n",
            " - 13s - loss: 1.4713 - accuracy: 0.3350 - val_loss: 1.4143 - val_accuracy: 0.3596\n",
            "Epoch 24/50\n",
            " - 13s - loss: 1.4713 - accuracy: 0.3362 - val_loss: 1.4162 - val_accuracy: 0.3692\n",
            "Epoch 25/50\n",
            " - 13s - loss: 1.4648 - accuracy: 0.3423 - val_loss: 1.4063 - val_accuracy: 0.3620\n",
            "\n",
            "Epoch 00025: saving model to jobdir/checkpoint.25.hdf5\n",
            "Epoch 26/50\n",
            "\n",
            "Evaluation epoch[25] (no checkpoints found)\n",
            " - 13s - loss: 1.4671 - accuracy: 0.3459 - val_loss: 1.4050 - val_accuracy: 0.3688\n",
            "Epoch 27/50\n",
            " - 13s - loss: 1.4631 - accuracy: 0.3424 - val_loss: 1.4094 - val_accuracy: 0.3644\n",
            "Epoch 28/50\n",
            " - 13s - loss: 1.4620 - accuracy: 0.3404 - val_loss: 1.4061 - val_accuracy: 0.3720\n",
            "Epoch 29/50\n",
            " - 13s - loss: 1.4597 - accuracy: 0.3416 - val_loss: 1.4050 - val_accuracy: 0.3684\n",
            "Epoch 30/50\n",
            " - 13s - loss: 1.4556 - accuracy: 0.3515 - val_loss: 1.3994 - val_accuracy: 0.3760\n",
            "\n",
            "Epoch 00030: saving model to jobdir/checkpoint.30.hdf5\n",
            "Epoch 31/50\n",
            "\n",
            "Evaluation epoch[30] (no checkpoints found)\n",
            " - 13s - loss: 1.4603 - accuracy: 0.3459 - val_loss: 1.4013 - val_accuracy: 0.3768\n",
            "Epoch 32/50\n",
            " - 13s - loss: 1.4553 - accuracy: 0.3489 - val_loss: 1.4003 - val_accuracy: 0.3740\n",
            "Epoch 33/50\n",
            " - 13s - loss: 1.4542 - accuracy: 0.3410 - val_loss: 1.3939 - val_accuracy: 0.3696\n",
            "Epoch 34/50\n",
            " - 13s - loss: 1.4547 - accuracy: 0.3461 - val_loss: 1.3922 - val_accuracy: 0.3776\n",
            "Epoch 35/50\n",
            " - 13s - loss: 1.4502 - accuracy: 0.3475 - val_loss: 1.3956 - val_accuracy: 0.3724\n",
            "\n",
            "Epoch 00035: saving model to jobdir/checkpoint.35.hdf5\n",
            "Epoch 36/50\n",
            "\n",
            "Evaluation epoch[35] (no checkpoints found)\n",
            " - 13s - loss: 1.4505 - accuracy: 0.3428 - val_loss: 1.3945 - val_accuracy: 0.3684\n",
            "Epoch 37/50\n",
            " - 13s - loss: 1.4464 - accuracy: 0.3544 - val_loss: 1.3887 - val_accuracy: 0.3812\n",
            "Epoch 38/50\n",
            " - 13s - loss: 1.4476 - accuracy: 0.3517 - val_loss: 1.3921 - val_accuracy: 0.3692\n",
            "Epoch 39/50\n",
            " - 13s - loss: 1.4470 - accuracy: 0.3553 - val_loss: 1.3888 - val_accuracy: 0.3788\n",
            "Epoch 40/50\n",
            " - 13s - loss: 1.4419 - accuracy: 0.3577 - val_loss: 1.3860 - val_accuracy: 0.3820\n",
            "\n",
            "Epoch 00040: saving model to jobdir/checkpoint.40.hdf5\n",
            "Epoch 41/50\n",
            "\n",
            "Evaluation epoch[40] (no checkpoints found)\n",
            " - 13s - loss: 1.4453 - accuracy: 0.3522 - val_loss: 1.3881 - val_accuracy: 0.3816\n",
            "Epoch 42/50\n",
            " - 13s - loss: 1.4445 - accuracy: 0.3513 - val_loss: 1.3827 - val_accuracy: 0.3796\n",
            "Epoch 43/50\n",
            " - 13s - loss: 1.4418 - accuracy: 0.3557 - val_loss: 1.3830 - val_accuracy: 0.3748\n",
            "Epoch 44/50\n",
            " - 13s - loss: 1.4418 - accuracy: 0.3546 - val_loss: 1.3846 - val_accuracy: 0.3784\n",
            "Epoch 45/50\n",
            " - 13s - loss: 1.4366 - accuracy: 0.3492 - val_loss: 1.3820 - val_accuracy: 0.3820\n",
            "\n",
            "Epoch 00045: saving model to jobdir/checkpoint.45.hdf5\n",
            "Epoch 46/50\n",
            "\n",
            "Evaluation epoch[45] (no checkpoints found)\n",
            " - 13s - loss: 1.4403 - accuracy: 0.3577 - val_loss: 1.3880 - val_accuracy: 0.3844\n",
            "Epoch 47/50\n",
            " - 13s - loss: 1.4358 - accuracy: 0.3594 - val_loss: 1.3847 - val_accuracy: 0.3724\n",
            "Epoch 48/50\n",
            " - 13s - loss: 1.4355 - accuracy: 0.3570 - val_loss: 1.3831 - val_accuracy: 0.3784\n",
            "Epoch 49/50\n",
            " - 13s - loss: 1.4353 - accuracy: 0.3589 - val_loss: 1.3842 - val_accuracy: 0.3768\n",
            "Epoch 50/50\n",
            " - 13s - loss: 1.4337 - accuracy: 0.3654 - val_loss: 1.3798 - val_accuracy: 0.3832\n",
            "\n",
            "Epoch 00050: saving model to jobdir/checkpoint.50.hdf5\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 126, 126, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 124, 124, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 62, 62, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 60, 60, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 57600)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               7372928   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 7,429,893\n",
            "Trainable params: 7,429,893\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXcExKeHA5XT",
        "colab_type": "text"
      },
      "source": [
        "# Additional metrics\n",
        "In the cells below, some further metrics and diagnostics are implemented. Such are confusion matrix, f-1 statistics and a model prediction tool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWJYzMmf3TjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('jobdir/retinopathy.hdf5')\n",
        "\n",
        "data = np.load(\"testDataSmall.npz\")\n",
        "X_test = data[\"X_test\"]\n",
        "Y_test = data[\"Y_test\"]\n",
        "\n",
        "yProbTest = model.predict_proba(X_test)\n",
        "yClassTest = model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMavLkOJ9GY4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e8e6920d-395a-4281-aa35-eea022aa5242"
      },
      "source": [
        "def convert_act(yInput):\n",
        "  if(yInput == [1, 0, 0, 0, 0]).all():\n",
        "    return 0\n",
        "  elif(yInput == [0, 1, 0, 0, 0]).all():\n",
        "    return 1\n",
        "  elif(yInput == [0, 0, 1, 0, 0]).all():\n",
        "    return 2\n",
        "  elif(yInput == [0, 0, 0, 1, 0]).all():\n",
        "    return 3\n",
        "  elif(yInput == [0, 0, 0, 0, 1]).all():\n",
        "    return 4\n",
        "\n",
        "val = input(\"Enter number of the test case: \") \n",
        "\n",
        "imgToShow = tf.keras.preprocessing.image.array_to_img(\n",
        "    X_test[int(val)],\n",
        "    data_format=None,\n",
        "    scale=True,\n",
        "    dtype=None\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "ax1 = plt.subplot2grid((15,9), (0,0), colspan=9, rowspan=9)\n",
        "ax2 = plt.subplot2grid((15,9), (9,2), colspan=5, rowspan=5)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.set_title(\"X_test[%s]\" % (val))\n",
        "ax1.imshow(imgToShow)\n",
        "    \n",
        "y_pos = np.arange(5)\n",
        "ax2.set_yticks(y_pos)\n",
        "ax2.set_yticklabels([0,1,2,3,4])\n",
        "ax2.set_xlabel('Probability')\n",
        "ax2.invert_yaxis()\n",
        "ax2.barh(y_pos, yProbTest[int(val)], xerr=0, align='center', color='green')\n",
        "\n",
        "print(\"Predicted Class = %s, Actuall Class = %s\" % (yClassTest[int(val)], convert_act(Y_test[int(val)])))\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter number of the test case: 5\n",
            "Predicted Class = 3, Actuall Class = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vOmpYge9MOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_Y_test = []\n",
        "\n",
        "for x in range(len(Y_test)):\n",
        "  new_Y_test.append(convert_act(Y_test[x]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g40ajA379S0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(new_Y_test, yClassTest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYHiSgzw9YAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8lBqcDx9ZK5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fe7eecd3-0f9d-4d98-ac2a-a100ee5283e3"
      },
      "source": [
        "cm_plot_labels = [0, 1, 2, 3, 4]\n",
        "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Tight layout not applied. tight_layout cannot make axes height small enough to accommodate all axes decorations\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1cbg0sz9bsP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f60d6807-45ad-46f2-97de-2037a3e2954b"
      },
      "source": [
        "print(classification_report(new_Y_test, yClassTest))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.30      0.32       500\n",
            "           1       0.35      0.42      0.38       500\n",
            "           2       0.32      0.25      0.28       500\n",
            "           3       0.35      0.31      0.33       500\n",
            "           4       0.49      0.64      0.56       500\n",
            "\n",
            "    accuracy                           0.38      2500\n",
            "   macro avg       0.37      0.38      0.37      2500\n",
            "weighted avg       0.37      0.38      0.37      2500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}