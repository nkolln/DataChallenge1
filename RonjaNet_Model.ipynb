{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RonjaNet Model",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25cV3M-FwWeq",
        "colab_type": "text"
      },
      "source": [
        "# RonjaNet Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoh_IZebGS0I",
        "colab_type": "text"
      },
      "source": [
        "## Importing and Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXmNOfkazaGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBnRS5K-JfWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import multiprocessing.pool\n",
        "import threading\n",
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "from functools import partial\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import layers, models\n",
        "from keras import metrics\n",
        "from keras.models import load_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import BatchNormalization, Activation, GlobalAveragePooling2D, MaxPool2D\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.backend import relu, sigmoid\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.saved_model import builder as saved_model_builder\n",
        "from tensorflow.python.saved_model import utils\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
        "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
        "from tensorflow.contrib.session_bundle import exporter\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from skimage import exposure, color\n",
        "import cv2 as cv\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Base Model\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def model_base(labels_dim):\n",
        "    \"\"\"Create a Keras Sequential model with layers.\"\"\"\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                     activation='relu',\n",
        "                     input_shape=(128, 128, 3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='glorot_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(labels_dim, activation='softmax', kernel_initializer='glorot_uniform'))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    compile_model(model)\n",
        "    return model\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# VGGNet Model\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def model_vgg19(labels_dim):\n",
        "    \n",
        "    model = VGG16(weights='imagenet', include_top=False, input_shape=(128,128,3))\n",
        "\n",
        "    flatten = Flatten()\n",
        "    new_layer2 = Dense(labels_dim, activation='softmax', name='my_dense_2')\n",
        "\n",
        "    inp2 = model.input\n",
        "    out2 = new_layer2(flatten(model.output))\n",
        "\n",
        "    model2 = Model(inp2, out2)\n",
        "    model2.summary(line_length=150)\n",
        "\n",
        "    compile_model(model2)\n",
        "    return model2\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# RonjaNet\n",
        "#-------------------------------------------------------------------------------\n",
        "def model_ronja(labels_dim):\n",
        "    \"\"\"Create a Keras Sequential model with layers.\"\"\"\n",
        "\n",
        "    model = models.Sequential([\n",
        "            \n",
        "    Conv2D(128, (3,3), input_shape=(128, 128, 3), strides=2, padding='same', activation='relu'),\n",
        "    Conv2D(128, (3,3), strides=2, padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Dropout(0.15), # makes RonjaNet slower, yet helps against overfitting\n",
        "\n",
        "    Conv2D(64, (3,3), strides=1, padding='same', activation='relu'),\n",
        "    Conv2D(64, (3,3), strides=1, padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Dropout(0.15),\n",
        "\n",
        "    Conv2D(28, (3,3), strides=1, padding='same', activation='relu'),\n",
        "    Conv2D(28, (3,3), strides=1, padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Dropout(0.15),\n",
        "\n",
        "    Conv2D(12, (3,3), strides=1, padding='same', activation='relu'),\n",
        "    Conv2D(12, (3,3), strides=1, padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Dropout(0.15),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(labels_dim, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    compile_model(model)\n",
        "    return model\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Compiling & Reading\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def compile_model(model):\n",
        "    opt = keras.optimizers.Adagrad(lr=0.01)\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def read_train_data():\n",
        "    start_time = time.time()\n",
        "    print(\"Start Read Train Data\")\n",
        "    data = np.load(\"trainDataSmall.npz\")\n",
        "    #data = np.load(\"/content/drive/My Drive/Colab Notebooks/trainDataSmall.npz\")\n",
        "    print(\"Train data read --- %s seconds ---\" % (time.time() - start_time))\n",
        "    X_train = data[\"X_train\"]\n",
        "    Y_train = data[\"Y_train\"]\n",
        "    print(\"Training - Total examples per class\", np.sum(Y_train, axis=0))\n",
        "    return [X_train, Y_train]\n",
        "\n",
        "\n",
        "def read_test_data():\n",
        "    start_time = time.time()\n",
        "    print(\"Start Read Test Data\")\n",
        "    data = np.load(\"testDataSmall.npz\")\n",
        "    #data = np.load(\"/content/drive/My Drive/Colab Notebooks/testDataSmall.npz\")\n",
        "    print(\"Test data read --- %s seconds ---\" % (time.time() - start_time))\n",
        "    X_test = data[\"X_test\"]\n",
        "    Y_test = data[\"Y_test\"]\n",
        "    print(\"Testing - Total examples per class\", np.sum(Y_test, axis=0))\n",
        "    \n",
        "    return [X_test, Y_test]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WgIF70v1ysJ",
        "colab_type": "text"
      },
      "source": [
        "# Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSuYY-albXsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASS_SIZE = 5\n",
        "FILE_PATH = 'checkpoint.{epoch:02d}.hdf5'\n",
        "RETINOPATHY_MODEL = 'retinopathy.hdf5'\n",
        "\n",
        "class ContinuousEval(keras.callbacks.Callback):\n",
        "    \"\"\"Continuous eval callback to evaluate the checkpoint once\n",
        "       every so many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 eval_frequency,\n",
        "                 job_dir):\n",
        "        self.eval_frequency = eval_frequency\n",
        "        self.job_dir = job_dir\n",
        "        [self.X_test, self.Y_test] = read_test_data()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        if epoch > 0 and epoch % self.eval_frequency == 0:\n",
        "            # Unhappy hack to work around h5py not being able to write to GCS.\n",
        "            # Force snapshots and saves to local filesystem, then copy them over to GCS.\n",
        "            model_path_glob = 'checkpoint.*'\n",
        "            model_path_glob = os.path.join(self.job_dir, model_path_glob)\n",
        "            checkpoints = glob.glob(model_path_glob)\n",
        "            if len(checkpoints) > 0:\n",
        "                checkpoints.sort()\n",
        "                retinopathy_model = load_model(checkpoints[-1])\n",
        "                retinopathy_model = compile_model(retinopathy_model)\n",
        "                loss, acc = retinopathy_model.evaluate(\n",
        "                    self.X_test, self.Y_test)\n",
        "                print('\\nEvaluation epoch[{}] metrics[{:.2f}, {:.2f}] {}'.format(\n",
        "                    epoch, loss, acc, retinopathy_model.metrics_names))\n",
        "\n",
        "            else:\n",
        "                print('\\nEvaluation epoch[{}] (no checkpoints found)'.format(epoch))\n",
        "\n",
        "def AHE(image):\n",
        "    image = exposure.equalize_adapthist(image, clip_limit=0.01)\n",
        "    return image\n",
        "\n",
        "def run():\n",
        "    tf.keras.backend.clear_session()\n",
        "    #local dir to write checkpoints and export model\n",
        "    job_dir = 'jobdir'\n",
        "    #Batch size for training steps\n",
        "    train_batch_size = 500\n",
        "    #Maximum number of epochs on which to train\n",
        "    num_epochs = 20\n",
        "    #Checkpoint per n training epochs\n",
        "    checkpoint_epochs = 5\n",
        "    #Perform one evaluation per n epochs\n",
        "    eval_frequency = 5\n",
        "\n",
        "    #DEFINE MODEL TO BE USED HERE\n",
        "    retinopathy_model = model_ronja(CLASS_SIZE)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(job_dir)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Unhappy hack to work around h5py not being able to write to GCS.\n",
        "    # Force snapshots and saves to local filesystem, then copy them over to GCS.\n",
        "    checkpoint_path = FILE_PATH\n",
        "    checkpoint_path = os.path.join(job_dir, checkpoint_path)\n",
        "\n",
        "    # Model checkpoint callback\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_path,\n",
        "        monitor='val_loss',\n",
        "        verbose=2,\n",
        "        period=checkpoint_epochs,\n",
        "        mode='max')\n",
        "\n",
        "    # Continuous eval callback\n",
        "    evaluation = ContinuousEval(eval_frequency,\n",
        "                                job_dir)\n",
        "\n",
        "    # Tensorboard logs callback\n",
        "    tblog = keras.callbacks.TensorBoard(\n",
        "        log_dir=os.path.join(job_dir, 'logs'),\n",
        "        histogram_freq=0,\n",
        "        write_graph=True,\n",
        "        embeddings_freq=0)\n",
        "\n",
        "    callbacks = [checkpoint, evaluation, tblog]\n",
        "\n",
        "    [X_train, Y_train] = read_train_data()\n",
        "    \n",
        "    datagen = ImageDataGenerator(\n",
        "        #preprocessing_function=AHE,\n",
        "        shear_range=0,\n",
        "        zoom_range=0,\n",
        "        horizontal_flip=True\n",
        "        ) # ideally this should be added to for improved performance\n",
        "    \n",
        "    history = retinopathy_model.fit_generator(\n",
        "        datagen.flow(X_train, Y_train, batch_size=train_batch_size),\n",
        "        steps_per_epoch=100,\n",
        "        epochs=num_epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose=2,\n",
        "        validation_data=(evaluation.X_test, evaluation.Y_test))\n",
        "\n",
        "    retinopathy_model.save(os.path.join(job_dir, RETINOPATHY_MODEL))\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Creating a plot for loss and accuracy for both training and validation set.\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    print(history.history.keys())\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'], 'bo')\n",
        "    plt.plot(history.history['val_accuracy'], 'b')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_acc', 'test_acc'], loc='upper left')\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'], 'bo')\n",
        "    plt.plot(history.history['val_loss'], 'b')\n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_loss', 'test_loss'], loc='upper left')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gGsDgLD_GY7l",
        "colab": {}
      },
      "source": [
        "\n",
        "## this will download the data from the internet, it may fail if the server is not up\n",
        "![ -f testDataSmall.npz ] || wget -O testDataSmall.npz \"https://www.win.tue.nl/~cdecampos/testDataSmall.npz\"\n",
        "![ -f trainDataSmall.npz ] || wget -O trainDataSmall.npz \"https://www.win.tue.nl/~cdecampos/trainDataSmall.npz\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw8aDmCZ2DUq",
        "colab_type": "text"
      },
      "source": [
        "# Run Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJz2tMXO2Dws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -fr jobdir/\n",
        "\n",
        "run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjDj53fKHC-d",
        "colab_type": "text"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0CwTRYoLpms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from numpy import loadtxt\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('jobdir/retinopathy.hdf5')\n",
        "data = np.load(\"testDataSmall.npz\")\n",
        "\n",
        "X_test = data[\"X_test\"]\n",
        "Y_test = data[\"Y_test\"]\n",
        "yProbTest = model.predict_proba(X_test)\n",
        "yClassTest = model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDxqnc4WLrt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_act(yInput):\n",
        "  if(yInput == [1, 0, 0, 0, 0]).all():\n",
        "    return 0\n",
        "\n",
        "  elif(yInput == [0, 1, 0, 0, 0]).all():\n",
        "    return 1\n",
        "\n",
        "  elif(yInput == [0, 0, 1, 0, 0]).all():\n",
        "    return 2\n",
        "\n",
        "  elif(yInput == [0, 0, 0, 1, 0]).all():\n",
        "    return 3\n",
        "    \n",
        "  elif(yInput == [0, 0, 0, 0, 1]).all():\n",
        "    return 4\n",
        "    \n",
        "val = input(\"Enter number of the test case: \")\n",
        "imgToShow = tf.keras.preprocessing.image.array_to_img(\n",
        "    X_test[int(val)],\n",
        "    data_format=None,\n",
        "    scale=True,\n",
        "    dtype=None)\n",
        "\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "ax1 = plt.subplot2grid((15,9), (0,0), colspan=9, rowspan=9)\n",
        "ax2 = plt.subplot2grid((15,9), (9,2), colspan=5, rowspan=5)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.set_title(\"X_test[%s]\" % (val))\n",
        "ax1.imshow(imgToShow)\n",
        "y_pos = np.arange(5)\n",
        "ax2.set_yticks(y_pos)\n",
        "ax2.set_yticklabels([0,1,2,3,4])\n",
        "ax2.set_xlabel('Probability')\n",
        "ax2.invert_yaxis()\n",
        "ax2.barh(y_pos, yProbTest[int(val)], xerr=0, align='center', color='green')\n",
        "print(\"Predicted Class = %s, Actual Class = %s\" % (yClassTest[int(val)], convert_act(Y_test[int(val)])))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJEKg7OsL0yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "new_Y_test = []\n",
        "\n",
        "for x in range(len(Y_test)):\n",
        "  new_Y_test.append(convert_act(Y_test[x]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOFSS9_-NOU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(new_Y_test, yClassTest)\n",
        "print(classification_report(new_Y_test, yClassTest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWg7w93PMmpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://scikit-learn.org/0.18/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7eUMyd1MpwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm_plot_labels = [0, 1, 2, 3, 4]\n",
        "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjvefCGRHO9V",
        "colab_type": "text"
      },
      "source": [
        "#### Attempt at Preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qx9CrCCDDi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "def AHE(image):\n",
        "    image = scipy.misc.toimage(image)\n",
        "\n",
        "    # create a CLAHE object (Arguments are optional).\n",
        "    lab = cv.cvtColor(image, cv.COLOR_RGB2LAB)\n",
        "\n",
        "    # split the image into L, A, and B channels\n",
        "    l_channel, a_channel, b_channel = cv.split(lab)\n",
        "\n",
        "    #Create and apply the CLAHE\n",
        "    clahe = cv.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n",
        "    l_channel = clahe.apply(l_channel)\n",
        "    l_channel.shape\n",
        "\n",
        "    # merge the CLAHE enhanced L channel with the original A and B channel\n",
        "    merged_channels = cv.merge((l_channel, a_channel, b_channel))\n",
        "\n",
        "    # convert iamge from LAB color model back to RGB color model\n",
        "    image = cv.cvtColor(merged_channels, cv.COLOR_LAB2BGR)\n",
        "    \n",
        "    image = exposure.equalize_adapthist(image, clip_limit=0.01)\n",
        "    return image\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PImrmmrnDg-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsYJEl15GWyh",
        "colab_type": "text"
      },
      "source": [
        "# Links\n",
        "\n",
        "\n",
        "## PreProcessing\n",
        "https://www.kaggle.com/tanlikesmath/training-on-previous-dataset-for-aptos\n",
        "\n",
        "https://www.kaggle.com/taindow/pre-processing-train-and-test-images\n",
        "\n",
        "https://www.kaggle.com/tanlikesmath/diabetic-retinopathy-resnet50-binary-cropped\n",
        "\n",
        "https://mc.ai/how-to-normalize-center-and-standardize-images-with-the-imagedatagenerator-in-keras/\n",
        "\n",
        "## Structure of CNN \n",
        "https://medium.com/datadriveninvestor/five-powerful-cnn-architectures-b939c9ddd57b\n",
        "\n",
        "https://www.machinecurve.com/index.php/2018/12/07/convolutional-neural-networks-and-their-components-for-computer-vision/#stacking-cnn-layers-into-an-architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2eX3EhFGY-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}