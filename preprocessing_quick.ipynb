{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of dc1-example.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkolln/DataChallenge1/blob/master/preprocessing_quick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBnRS5K-JfWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import itertools\n",
        "import multiprocessing.pool\n",
        "import threading\n",
        "from functools import partial\n",
        "from keras.models import load_model\n",
        "from keras.models import Model\n",
        "import keras\n",
        "import pandas as pd\n",
        "from keras import backend as K\n",
        "from keras import layers, models\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.backend import relu, sigmoid\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.saved_model import builder as saved_model_builder\n",
        "from tensorflow.python.saved_model import utils\n",
        "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
        "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
        "from tensorflow.contrib.session_bundle import exporter\n",
        "import os\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "\n",
        "def model_vgg19(labels_dim):\n",
        "    \n",
        "    model = VGG16(weights='imagenet', include_top=False, input_shape=(128,128,3))\n",
        "\n",
        "    flatten = Flatten()\n",
        "    new_layer2 = Dense(labels_dim, activation='softmax', name='my_dense_2')\n",
        "\n",
        "    inp2 = model.input\n",
        "    out2 = new_layer2(flatten(model.output))\n",
        "\n",
        "    model2 = Model(inp2, out2)\n",
        "    model2.summary(line_length=150)\n",
        "\n",
        "    compile_model(model2)\n",
        "    return model2\n",
        "\n",
        "\n",
        "def model_fn(labels_dim):\n",
        "    \"\"\"Create a Keras Sequential model with layers.\"\"\"\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                     activation='relu',\n",
        "                     input_shape=(128, 128, 3)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='glorot_uniform'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='glorot_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(labels_dim, activation='softmax', kernel_initializer='glorot_uniform'))\n",
        "\n",
        "    compile_model(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def compile_model(model):\n",
        "    opt = keras.optimizers.Adagrad(lr=0.01)\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def read_train_data():\n",
        "    start_time = time.time()\n",
        "    print(\"Start Read Train Data\")\n",
        "    data = np.load(\"trainDataSmall.npz\")\n",
        "    #data = np.load(\"/content/drive/My Drive/Colab Notebooks/trainDataSmall.npz\")\n",
        "    print(\"Train data read --- %s seconds ---\" % (time.time() - start_time))\n",
        "    X_train = data[\"X_train\"]\n",
        "    Y_train = data[\"Y_train\"]\n",
        "    print(\"Training - Total examples per class\", np.sum(Y_train, axis=0))\n",
        "    return [X_train, Y_train]\n",
        "\n",
        "\n",
        "def read_test_data():\n",
        "    start_time = time.time()\n",
        "    print(\"Start Read Test Data\")\n",
        "    data = np.load(\"testDataSmall.npz\")\n",
        "    #data = np.load(\"/content/drive/My Drive/Colab Notebooks/testDataSmall.npz\")\n",
        "    print(\"Test data read --- %s seconds ---\" % (time.time() - start_time))\n",
        "    X_test = data[\"X_test\"]\n",
        "    Y_test = data[\"Y_test\"]\n",
        "    print(\"Testing - Total examples per class\", np.sum(Y_test, axis=0))\n",
        "    \n",
        "    return [X_test, Y_test]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d20xpcloBWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "![ -f testDataSmall.npz ] || wget -O testDataSmall.npz \"https://www.win.tue.nl/~cdecampos/testDataSmall.npz\"\n",
        "![ -f trainDataSmall.npz ] || wget -O trainDataSmall.npz \"https://www.win.tue.nl/~cdecampos/trainDataSmall.npz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GhB53KZTdhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "def AHE(image):\n",
        "    #datagen1.standardize(image)\n",
        "    image = exposure.equalize_adapthist(image, clip_limit=0.01)\n",
        "    \n",
        "                    \n",
        "datagen1 = ImageDataGenerator(\n",
        "        shear_range=0,\n",
        "        zoom_range=0,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function=AHE\n",
        "        )\n",
        "\n",
        "class ContinuousEval(keras.callbacks.Callback):\n",
        "    \"\"\"Continuous eval callback to evaluate the checkpoint once\n",
        "       every so many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 eval_frequency,\n",
        "                 job_dir):\n",
        "        self.eval_frequency = eval_frequency\n",
        "        self.job_dir = job_dir\n",
        "        [self.X_test, self.Y_test] = read_test_data()\n",
        "        datagen1.fit(self.X_test)\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        if epoch > 0 and epoch % self.eval_frequency == 0:\n",
        "            # Unhappy hack to work around h5py not being able to write to GCS.\n",
        "            # Force snapshots and saves to local filesystem, then copy them over to GCS.\n",
        "            model_path_glob = 'checkpoint.*'\n",
        "            model_path_glob = os.path.join(self.job_dir, model_path_glob)\n",
        "            checkpoints = glob.glob(model_path_glob)\n",
        "            if len(checkpoints) > 0:\n",
        "                checkpoints.sort()\n",
        "                retinopathy_model = load_model(checkpoints[-1])\n",
        "                retinopathy_model = compile_model(retinopathy_model)\n",
        "                loss, acc = retinopathy_model.evaluate(\n",
        "                    self.X_test, self.Y_test)\n",
        "                print('\\nEvaluation epoch[{}] metrics[{:.2f}, {:.2f}] {}'.format(\n",
        "                    epoch, loss, acc, retinopathy_model.metrics_names))\n",
        "            else:\n",
        "                print('\\nEvaluation epoch[{}] (no checkpoints found)'.format(epoch))\n",
        "# Continuous eval callback\n",
        "evaluation = ContinuousEval(5,\n",
        "                            'job_dir')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "validation=(evaluation.X_test, evaluation.Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSuYY-albXsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This code implements a Feed forward neural network using Keras API.\"\"\"\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage import exposure, color\n",
        "\n",
        "CLASS_SIZE = 5\n",
        "FILE_PATH = 'checkpoint.{epoch:02d}.hdf5'\n",
        "RETINOPATHY_MODEL = 'retinopathy.hdf5'\n",
        "\n",
        "\n",
        "datagen1 = ImageDataGenerator(\n",
        "    shear_range=0,\n",
        "    zoom_range=0,\n",
        "    horizontal_flip=True\n",
        "    )\n",
        "\n",
        "def AHE(image):\n",
        "    #datagen1.standardize(image)\n",
        "    image = exposure.equalize_adapthist(image, clip_limit=0.01)\n",
        "    \n",
        "\n",
        "    return image\n",
        "\n",
        "def run():\n",
        "    tf.keras.backend.clear_session()\n",
        "    #local dir to write checkpoints and export model\n",
        "    job_dir = 'jobdir'\n",
        "    #Batch size for training steps\n",
        "    train_batch_size = 100\n",
        "    #Maximum number of epochs on which to train\n",
        "    num_epochs = 30\n",
        "    #Checkpoint per n training epochs\n",
        "    checkpoint_epochs = 5\n",
        "    #Perform one evaluation per n epochs\n",
        "    eval_frequency = 5\n",
        "\n",
        "    retinopathy_model = model_fn(CLASS_SIZE)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(job_dir)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Unhappy hack to work around h5py not being able to write to GCS.\n",
        "    # Force snapshots and saves to local filesystem, then copy them over to GCS.\n",
        "    checkpoint_path = FILE_PATH\n",
        "    checkpoint_path = os.path.join(job_dir, checkpoint_path)\n",
        "\n",
        "    # Model checkpoint callback\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_path,\n",
        "        monitor='val_loss',\n",
        "        verbose=2,\n",
        "        period=checkpoint_epochs,\n",
        "        mode='max')\n",
        "\n",
        "    \n",
        "\n",
        "    # Tensorboard logs callback\n",
        "    tblog = keras.callbacks.TensorBoard(\n",
        "        log_dir=os.path.join(job_dir, 'logs'),\n",
        "        histogram_freq=0,\n",
        "        write_graph=True,\n",
        "        embeddings_freq=0)\n",
        "\n",
        "    callbacks = [checkpoint, evaluation, tblog]\n",
        "\n",
        "    [X_train, Y_train] = read_train_data()\n",
        "    datagen1.fit(X_train)\n",
        "    \n",
        "    datagen = ImageDataGenerator(\n",
        "        shear_range=0,\n",
        "        zoom_range=0,\n",
        "        horizontal_flip=True\n",
        "        )\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    retinopathy_model.fit_generator(\n",
        "        datagen.flow(X_train, Y_train, batch_size=train_batch_size),\n",
        "        steps_per_epoch=100,\n",
        "        epochs=num_epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose=2,\n",
        "        validation_data=validation)\n",
        "\n",
        "    retinopathy_model.save(os.path.join(job_dir, RETINOPATHY_MODEL))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkGuHkYwNUen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGTkEsUfwPkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APgL3CRUn_su",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gGsDgLD_GY7l",
        "colab": {}
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive') ## If you want to read from your drive - this is not required, as you can get the files as below, but it might faster\n",
        "\n",
        "## this will download the data from the internet, it may fail if the server is not up\n",
        "#![ -f testDataSmall.npz ] || wget -O testDataSmall.npz \"https://www.win.tue.nl/~cdecampos/testDataSmall.npz\"\n",
        "#![ -f trainDataSmall.npz ] || wget -O trainDataSmall.npz \"https://www.win.tue.nl/~cdecampos/trainDataSmall.npz\"\n",
        "!rm -fr jobdir/\n",
        "\n",
        "run()\n",
        "#drive.flush_and_unmount() ## if you need to unmount your google drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J2U0PCJwOaX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hivaPLEW3eyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "   pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jZ7nljf6TV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1O8YvSricY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}